{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9a5e7c-b33a-49f2-86d4-878f48de092f",
   "metadata": {},
   "source": [
    "# Modellierung der MPA-Sensordaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524a8b24-d414-4a18-9e35-7398f820c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import yaml\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "from helper.feature_extract import extract_highest_amplitude_features_with_mp, get_all_sensors_in_df, feature_extractor_wrapper\n",
    "from helper.plot import plot_residuals, plot_error_per_cat\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from IPython.display import HTML\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b54df-c8b1-4464-b732-930dab8c4208",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cacf2f5e-e894-44a8-a1f2-14d65e1d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def add_outlier_feature_length(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adds new Feature declaring outliers\"\"\"\n",
    "    # Calculate prediction for outlier\n",
    "    max_len_col = [col for col in df.columns if 'max_len' in col][0]\n",
    "    length_transformed = np.log(df[max_len_col])\n",
    "    length_transformed_std = (length_transformed - np.mean(length_transformed)) / np.std(length_transformed)\n",
    "    outlier_prediction = length_transformed_std > 3\n",
    "    \n",
    "    # add to df\n",
    "    df['outlier'] = outlier_prediction\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_all_studies_df(studies: list) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates all optuna studies\"\"\"\n",
    "    df = pd.concat([study.trials_dataframe() for study in studies], axis=0)\n",
    "    df = df.sort_values(by='value', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_studies_from_folder(folder_name: str, device_name: str) -> list:\n",
    "    \"\"\"Loads studies from a given folder with Device Name\"\"\"\n",
    "    # Load study\n",
    "    files = [f for f in os.listdir(folder_name) if device_name in f]\n",
    "    studies= []\n",
    "    for i, file in enumerate(files):\n",
    "        filename = folder_name + file \n",
    "        with open(filename, 'rb') as pkl_file:\n",
    "            study_pkl = pickle.load(pkl_file)\n",
    "            studies.append(study_pkl)\n",
    "    \n",
    "    return studies\n",
    "\n",
    "def plot_parallel_cordinates(df: pd.DataFrame, objective_maximize=True, columns=None):\n",
    "    \"\"\"Plots Opt. History as Parallel Cordinates\"\"\"\n",
    "    if columns:\n",
    "        columns.append('number')\n",
    "        columns.append('value')\n",
    "        df = df[columns]\n",
    "        \n",
    "    cols = [col.split('_') for col in df.columns]\n",
    "    df.columns = ['_'.join(col[1:])  if len(col) > 1 else col[0] for col in cols] \n",
    "    \n",
    "    if objective_maximize:\n",
    "        fig = px.parallel_coordinates(data_frame=df.drop('number', axis=1), color='value', color_continuous_scale='Blues', \n",
    "                                      height=500, width=1500)\n",
    "    else:\n",
    "        fig = px.parallel_coordinates(data_frame=df.drop('number', axis=1), color='value', color_continuous_scale='Blues_r', \n",
    "                                      height=500, width=1500)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043efc5-3300-408b-841c-6ae30419dfde",
   "metadata": {},
   "source": [
    "# Hyperparam Optimization CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5237c0-eb60-4e0f-84b5-e8faecea82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_r2(trial):\n",
    "    X_transformed = X_train.copy()\n",
    "    \n",
    "    # Pre-Processing\n",
    "    ## Polynomial Features\n",
    "    add_outlier = trial.suggest_categorical('outlier_feature', [True, False])    \n",
    "    if add_outlier:\n",
    "        X_transformed = add_outlier_feature_length(X_transformed)\n",
    "    \n",
    "    ## Feature Transformation to normalize data\n",
    "    apply_feature_transformation = trial.suggest_categorical('apply_feature_transformation', [True, False])\n",
    "    if apply_feature_transformation:\n",
    "        transformer = PowerTransformer(standardize=trial.suggest_categorical('pt_standardize', [True, False]))\n",
    "        X_transformed = transformer.fit_transform(X_train)\n",
    "        \n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 5000),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['RMSE', 'MAPE']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.5),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 1e0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.01, 1),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 2, 20),\n",
    "        'one_hot_max_size': trial.suggest_int('one_hot_max_size', 2, 20), \n",
    "        'silent': True\n",
    "    }\n",
    "    # Conditional Hyper-Parameters\n",
    "    if param['bootstrap_type'] == 'Bayesian':\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "    elif param['bootstrap_type'] == 'Bernoulli':\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n",
    "\n",
    "    reg = CatBoostRegressor(**param, thread_count=6)\n",
    "    score = cross_val_score(estimator=reg, X=X_train,\n",
    "                            y=y_train, cv=10, n_jobs=4)    \n",
    " \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09225d1e-0f63-458d-bee2-1951a68d30f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'results' Skipping iteration\n",
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'results/final' Skipping iteration\n",
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'results/final/' Skipping iteration\n",
      "Config already exists!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Want Overwrite Existing? (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Study for mpa ==========\n",
      "INFO || Extracting Mean Features\n",
      "INFO || Extracting Max Features for types: ['M01', 'M02', 'M04', 'M03']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-21 16:41:39,742]\u001b[0m A new study created in memory with name: no-name-e40059dc-eb98-42fa-bf73-d87ca66dcd49\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | Split Data X, y ...\n",
      "INFO | Train-Test Split ...\n",
      "INFO | Tune Model ...\n",
      "==============================  Process Finished  ==============================\n"
     ]
    }
   ],
   "source": [
    "# './data/data_mpa.txt', './data/data_spg.txt', './data/data_sps.txt'\n",
    "config = dict(\n",
    "    FILEPATHS = ['../data/data_mpa.txt', '../data/data_spg.txt', \n",
    "                 '../data/data_sps.txt'],\n",
    "    TUNING_ITER = 3,\n",
    "    N_TRIALS = 100,\n",
    "    DROP_COLUMNS = ['velocity', 'start_time', 'packnr'],\n",
    "    LOG_SCALE_TARGET = False,\n",
    "    MODEL_NAME = 'LGBM',\n",
    "    SAVE_DIR = './results/final/' , # Needs to end with '/' like './results/final/'\n",
    "    EXTRACT_MAX_FEATURES = True,\n",
    "    EXTRACT_MEAN_FEATURES = True,\n",
    "    DEBUG_RUN = True,\n",
    "    DATE_FROM = 2020,  # selcts Dates >= DATE_FROM\n",
    ")\n",
    "\n",
    "try:\n",
    "    save_dir = config['SAVE_DIR'].split('/')[1:]\n",
    "    check_dir = None\n",
    "    for sdir in save_dir:\n",
    "        if check_dir:\n",
    "            check_dir = check_dir + '/' + sdir\n",
    "        else:\n",
    "            check_dir = sdir\n",
    "        try:\n",
    "            os.mkdir(check_dir)\n",
    "        except FileExistsError as fe:\n",
    "            print(fe, 'Skipping iteration')\n",
    "except:\n",
    "    os.mkdir(config['SAVE_DIR'])\n",
    "\n",
    "# Save Parametr-configs of file\n",
    "if 'config.yaml' in os.listdir(config['SAVE_DIR']):\n",
    "    print(Exception('Config already exists!'))\n",
    "    if input('Want Overwrite Existing? (y/n)')=='y':\n",
    "        with open(config['SAVE_DIR'] + 'config.yaml', 'w') as yaml_file:\n",
    "            yaml.dump(config, yaml_file)\n",
    "else:\n",
    "    with open(config['SAVE_DIR'] + 'config.yaml', 'w') as yaml_file:\n",
    "        yaml.dump(config, yaml_file)\n",
    "        \n",
    "for path in config['FILEPATHS']:\n",
    "    \n",
    "    device_name = path.split('_')[-1].split('.')[0]\n",
    "\n",
    "    print(10*'=', f'Starting Study for {device_name}', 10*'=')\n",
    "    # Read processed data\n",
    "    if config['DEBUG_RUN']:\n",
    "        data = pd.read_table(path, sep=' ', nrows=1000)\n",
    "    else:\n",
    "        data = pd.read_table(path, sep=' ')\n",
    "        \n",
    "    # Resample data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)        \n",
    "    \n",
    "    # Extract Features\n",
    "    data = feature_extractor_wrapper(df=data, extract_max_features=config['EXTRACT_MAX_FEATURES'],\n",
    "                                     extract_mean_features=config['EXTRACT_MEAN_FEATURES'])\n",
    "    #Filtering data\n",
    "    if config['DATE_FROM'] and not config['DEBUG_RUN']:\n",
    "        data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "        data = data.loc[data['start_time'].dt.year >= config['DATE_FROM']]\n",
    "            \n",
    "    # Splitting of Data\n",
    "    print('INFO | Split Data X, y ...')\n",
    "    feature_cols = data.columns.to_list()\n",
    "    feature_cols.remove('size_mm')\n",
    "    for col in config['DROP_COLUMNS']:\n",
    "        feature_cols.remove(col)\n",
    "    \n",
    "    # Split row wise and Target-wise\n",
    "    data, holdout = train_test_split(data, test_size=.1)\n",
    "    X_train, y_train = data[feature_cols], data['size_mm']\n",
    "\n",
    "    print('INFO | Train-Test Split ...')\n",
    "    if config['LOG_SCALE_TARGET']:\n",
    "        y_train = np.log(y_train)\n",
    "\n",
    "    print('INFO | Tune Model ...')\n",
    "    for i in range(config['TUNING_ITER']):\n",
    "        if config['DEBUG_RUN']:\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective_r2, n_trials=0, n_jobs=1, timeout=5)\n",
    "            break\n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective_r2, n_trials=config['N_TRIALS'], n_jobs=1)\n",
    "            # Save best params\n",
    "            study_name = '_'.join([config['MODEL_NAME'], device_name, str(i)])\n",
    "\n",
    "            with open(f'{config[\"SAVE_DIR\"]}{study_name}.pkl', 'wb') as pkl_file:\n",
    "                pickle.dump(study, pkl_file)\n",
    "    if config['DEBUG_RUN']:\n",
    "        break\n",
    "print(30*'=', ' Process Finished ', 30*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048165b0-756d-462b-890c-f48992829dc2",
   "metadata": {},
   "source": [
    "# Investigate Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2968ad2-2337-4a65-a07f-e4fe38864f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import plotly.express as px\n",
    "from plot import plot_residuals, plot_error_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7933a4f-9eb7-44b3-868c-3b2275dcdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name ='./results/20211220_WV_ZCR_PV_RMSE_STAT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88029f4e-5e09-4502-9c6a-c85b7a72e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_name + 'config.yaml', 'r') as yaml_file:\n",
    "    configs = yaml.load(yaml_file)\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd35e9b-60d5-4171-8ba8-46bc648ee36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_studies_from_folder(folder_name=folder_name, device_name='mpa')\n",
    "df = concat_all_studies_df(df)\n",
    "df = df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff7f17-28f5-4ada-b5b0-3a1213beb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc34d9-96c8-41ca-8305-d88501b8d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_parallel_cordinates(df, objective_maximize=True)\n",
    "HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92bbf1-aed8-4b79-9f7a-3fd607d4d6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
